{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304b83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,json\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a59cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hooks.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75322a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.checkpoints import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e8f5c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from src.utils.train_logger import FileTrainLogger\n",
    "from src.dataio.dataset import DynamicItemDataset\n",
    "from src.dataio.batch import PaddedBatch #padded_keys\n",
    "from functools import partial\n",
    "\n",
    "logger = FileTrainLogger(\n",
    "    './log.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "97418de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch\n",
    "\n",
    "class SimpleBrain(Brain):\n",
    "    \n",
    "    def compute_forward(self, batch, stage):\n",
    "#         print(batch.__dict__,) \n",
    "        batch = batch.to(self.device)\n",
    "#         print(batch.x_train.data.size(), batch.y_train.data.size())\n",
    "        return self.modules.model(batch.x_train.data)\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "#         print(batch.id, batch.x_train, batch.y_train)\n",
    "#         print(batch[1].size(),predictions.size())\n",
    "        \n",
    "        self.loss_metric.append(\n",
    "             batch.id, predictions, batch.y_train.data\n",
    "        )\n",
    "\n",
    "        # Compute classification error at test time\n",
    "        if stage != src.hooks.core.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, batch.y_train.data,)\n",
    "            \n",
    "        return torch.nn.functional.l1_loss(predictions, batch.y_train.data)\n",
    "    \n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\"Gets called at the beginning of each epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = utils.metric_stats.Metric(\n",
    "            metric= torch.nn.functional.l1_loss\n",
    "        )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != src.hooks.core.Stage.TRAIN:\n",
    "            self.error_metrics = utils.metric_stats.Metric(\n",
    "            metric= torch.nn.functional.l1_loss\n",
    "        )\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage ==  src.hooks.core.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        else:\n",
    "            stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"error\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage ==  src.hooks.core.Stage.VALID:\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            logger.log_stats(\n",
    "                {\"Epoch\": epoch,},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
    "\n",
    "        \n",
    "            \n",
    "model = torch.nn.Linear(in_features=10, out_features=10)\n",
    "checkpoint = Checkpointer(\"./\")\n",
    "brain = SimpleBrain({\"model\": model}, opt_class=lambda x: SGD(x, 0.1),checkpointer=checkpoint)\n",
    "# brain.fit(range(1), ([torch.rand(10, 10), torch.rand(10, 10)],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c1e6497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.rand(1000, 10)\n",
    "y_train  = X_train*2 + 3\n",
    "X_test = torch.rand(100, 10)\n",
    "y_test  = X_test*2 + 3\n",
    "\n",
    "data_set = {\n",
    "    i:{'x_train':k[0],'y_train':k[1]} for i,k in enumerate(zip(X_train,y_train))\n",
    "}\n",
    "val_set = {\n",
    "    i:{'x_val':k[0],'y_val':k[1]} for i,k in enumerate(zip(X_test,y_test))\n",
    "}\n",
    "\n",
    "@utils.data_pipeline.takes(\"x_train\",\"y_train\")\n",
    "@utils.data_pipeline.provides(\"x_train1\",\"y_train1\",\"double\")\n",
    "def audio_pipeline(x_train, y_train):\n",
    "    \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
    "    This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "#     sig = sb.dataio.dataio.read_audio(wav)\n",
    "    return x_train+1, y_train+2, y_train *2\n",
    "\n",
    "# # Define label pipeline:\n",
    "# @sb.utils.data_pipeline.takes(\"spk_id\")\n",
    "# @sb.utils.data_pipeline.provides(\"spk_id\", \"spk_id_encoded\")\n",
    "# def label_pipeline(spk_id):\n",
    "#     yield spk_id\n",
    "#     spk_id_encoded = label_encoder.encode_label_torch(spk_id)\n",
    "#     yield spk_id_encoded\n",
    "\n",
    "# Define datasets. We also connect the dataset with the data processing\n",
    "# functions defined above.\n",
    "dataset = DynamicItemDataset(data_set, dynamic_items=[audio_pipeline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "131c5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_output_keys(['id','x_train','y_train','double'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6aa82b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'x_train': tensor([0.5618, 0.4003, 0.6037, 0.6277, 0.2039, 0.3526, 0.0206, 0.8732, 0.9231,\n",
       "         0.0463]),\n",
       " 'y_train': tensor([4.1237, 3.8005, 4.2074, 4.2553, 3.4078, 3.7052, 3.0412, 4.7464, 4.8461,\n",
       "         3.0925]),\n",
       " 'double': tensor([8.2474, 7.6011, 8.4148, 8.5106, 6.8156, 7.4104, 6.0825, 9.4928, 9.6923,\n",
       "         6.1850])}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "52781112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 215.96it/s, train_loss=3.37]\n",
      "100%|██████████| 32/32 [00:00<00:00, 513.18it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 183.66it/s, train_loss=2.27]\n",
      "100%|██████████| 32/32 [00:00<00:00, 390.67it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 217.48it/s, train_loss=1.22]\n",
      "100%|██████████| 32/32 [00:00<00:00, 459.99it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 236.47it/s, train_loss=0.626]\n",
      "100%|██████████| 32/32 [00:00<00:00, 462.24it/s]\n"
     ]
    }
   ],
   "source": [
    "brain.fit(range(4), dataset,\n",
    "          valid_set= dataset,\n",
    "         train_loader_kwargs={'batch_size':32},\n",
    "        valid_loader_kwargs={'batch_size':32},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf43084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
